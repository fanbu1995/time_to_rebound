---
title: "B2 Rebound Analysis Update"
author: "Fan Bu"
date: "07/07/2020"
output: slidy_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "~/Documents/Research_and_References/HIV_rebound_summer2020/")
knitr::opts_chunk$set(fig.width=8, fig.height=6, fig.align = "center")
```

```{r setup 2, echo=FALSE, message=F, warning=F}
library(tidyverse)
library(survival)
library(survminer)
library(glmnet)

```

## Overview

Main updates

- Added more "animal information": Sex, challenge numbers, dosage
- Adjusted for FDR (using q-values) of the univariate Cox PH models
- Re-ran Cox PH "Lasso" model with manually selected predictors (took out the ones with high correlation to avoid colinearity)
- Literature review (on statistical inference of dynamical models)

## Investigating more predictors

```{r load data, echo=FALSE}
dat_log = readRDS("reboundB2_logTrans_CellCounts_AnimalInfo.rds")
```

The updated list of potential predictors:

- **Animal characteristics**: Sex, A01, challenge number, dosage
- **Viral load pre ART**: peak viral load, viral load at treatment & area-under-VL-curve
- **Antibody response**:
    - ELISA: GP41 and GP120, peak level and level at treatment
    - Neutralization: Point IC50 & Pos AUC measured at 0, 2, 4, 8 weeks post ATI
- **Cell-associated RNA and DNA**: 
    - SHIV RNA copies per million cell RNA: measured in blood, lymph node and rectal biopsy at 8 and 56 weeks post infection (RB only at 56 weeks)
    - SHIV DNA copies per million cell DNA: measured in blood, lymph node and rectal biopsy at 8, 16, 36 and 56 weeks post infection (no RB at 8 weeks)
- **Absolute CD4 and CD8 cell counts**:
    - Estimated using flow cytometry and CBC data
    - "Measurements" at 0, 4, 8 weeks

    
## On animal characteristics

- Sex and A01 (both are factor variables): no significant effect on time to rebound
- Challenge number and dosage: not very high concordance with order to rebound
    - challeng number: $C = 0.625$ (ranked #11)
    - dosage: $C=0.579$ (ranked #15)
  
Below is the KM curve stratified by Sex. The univariate Cox PH model fitted with `Sex` as the predictor doesn't suggest any significant effect either ($p=0.4$ by LRT).

```{r KM curve by Sex, echo=FALSE}
Sex_KM = survfit(Surv(rebound_time_days_post_ati, observed)~Sex, 
                 data=dat_log)
## prettier versions
ggsurvplot(Sex_KM, data = dat_log, 
           censor.shape="|", censor.size = 4,
           size = 1.5, palette = c("#E7B800", "#2E9FDF"),
           conf.int = FALSE,
           ggtheme = theme_bw())
```

## Adjust for FDR (univariate models)
A univriate model is fitted for each of the potential predictors (updated), and for each model, the **q-value** (or "FDR", proposed by Benjamini & Hochberg (1995)) is computed based on the p-value of likelihood-ratio-test for significance. 

The only predictor with a FDR $<0.2$ is `pos_auc_0_weeks_post_ATI`, with a q-value of $0.09284$.

## "CoxNet" with pre-selected predictors

A "candidate pool" of potential predictors is manually selected after examining the pairwise correlation between all predictors in the dataset. 

<!-- (The general selection criterion is to drop one of two variables if the correlation is $>0.6$ - this is not carried out very strictly as the goal is simply to avoid too much collinearity). -->

<!-- After the manual selection, the number of numerical predictors is reduced from $39$ to $15$. -->

(The general selection criterion is to drop one of two variables if the correlation is $>0.8$ - this is not carried out very strictly as the goal is simply to avoid too much collinearity).

After the manual selection, the number of numerical predictors is reduced from $39$ to $23$.

A "best model" is obtained via 5-fold cross validation (again, the outcome isn't very stable). 

```{r coxnet, echo=F}
# 4.1 put together a dataset with "manually selected" predictors

dat_log_sel = dat_log %>% 
  select(rebound_time_days_post_ati, observed,
         log_peak_vl_2, log_vl_treat,
         log_peak_gp41, log_peak_gp120,
         log_RNA_copies_blood_56, log_RNA_copies_LN_56,
         log_RNA_copies_RB_56, 
         log_DNA_copies_Blood_16:log_DNA_copies_Blood_56,
         log_DNA_copies_LN_36, log_DNA_copies_LN_56,
         log_DNA_copies_RB_16:log_DNA_copies_RB_56,
         pos_auc_0_weeks_post_ATI, pos_auc_4_weeks_post_ATI,
         pos_auc_8_weeks_post_ATI,
         log_Abs_CD4_week0:log_Abs_CD4_week8,
         Challenge_times, Dosage)

## right now: 23 predictors
X = dat_log_sel[,3:25] %>% as.matrix()


# 4.2 fit Lasso (alpha=1)
phmod_lasso = glmnet(X, Surv(dat_log_sel$rebound_time_days_post_ati, 
                          dat_log_sel$observed),
                     family = "cox")

# (cross validation)
## (using partial likelihood, 5-fold)
cv_phmod_lasso1 = cv.glmnet(X, 
                            Surv(dat_log_sel$rebound_time_days_post_ati, 
                                 dat_log_sel$observed),
                            family = "cox", nfolds = 5)
#plot(cv_phmod_lasso1)
### somewhere between 1 and 4 predictors...
### BUT! CV with 10 observations isn't very reliable
### Probably can only get "promising predictors"

## (using concordance, 5-fold)
cv_phmod_lasso2 = cv.glmnet(X, 
                            Surv(dat_log_sel$rebound_time_days_post_ati, 
                                 dat_log_sel$observed),
                            family = "cox", nfolds = 5,
                            type.measure = "C")
plot(cv_phmod_lasso2)


## extract coefficients using the 2st CV results
# cv_phmod_lasso2$lambda.min
# [1] 0.3589552
coefficients <- coef(phmod_lasso, s = cv_phmod_lasso2$lambda.min)
active_index <- which(coefficients != 0)
active_coefficients <-coefficients[active_index]
active_predictors = attr(coefficients,"Dimnames")[[1]][active_index]

### put together a table for this result
cv_res = data.frame(Predictor = active_predictors, 
                    Coefficient = active_coefficients)
knitr::kable(cv_res, digits = 4)
```

## "CoxNet" with pre-selected predictors (Cont'd)

Same as last time, we can also get a "predictor inclusion ranking table" from the model outcomes.

Here in row $n$, we also provide the multivariate concordance using predictors $1,2,\ldots,n$. Understandably, concordance can reach 1 (or very close to 1) when we add more and more predictors, but this is not desired as a super good fit to the data often leads to overfitting; we may want to stop at 4 or even less predictors to be safe.

```{r coxnet predictor ranking, echo=F, warning=F, message=F}
# 4.3 Use Lasso results to obtain a "predictor inclusion ranking"
pred_in = NULL
coef_sign = NULL

for(l in cv_phmod_lasso1$lambda){
  coefficients = coef(phmod_lasso, s = l)
  active_index = which(coefficients != 0)
  active_coefficients = coefficients[active_index]
  active_predictors = attr(coefficients,"Dimnames")[[1]][active_index]
  
  #cat(active_predictors, "\n")
  
  if(any(!active_predictors %in% pred_in)){
    new_index = which(!active_predictors %in% pred_in)
    pred_in = c(pred_in, active_predictors[new_index])
    new_coef_sign = ifelse(active_coefficients[new_index] > 0, 
                           "+", "-")
    coef_sign = c(coef_sign, new_coef_sign)
  }
}

## get a vector of "effect on rebound"
## (+: accelerate; -: delay)
rebound_effect = sapply(coef_sign, 
                        function(x) ifelse(x=="+","accelerate","delay")) %>%
  as.vector()

## also get a vector of (univariate) concordance
# Response = "Surv(rebound_time_days_post_ati, observed)"
# All_covars = pred_in
# 
# C_stats = NULL
# 
# for(v in All_covars){
#   f = as.formula(paste(Response,v,sep = " ~ "))
#   C_v = concordance(f, data=dat_log_sel, timewt = "n")$concordance
#   C_stats = c(C_stats, C_v)
# }

## 06/29/2020
## get a vector of multivariate (cumulative concordance)
Response = "Surv(rebound_time_days_post_ati, observed)"

Cum_C_stats = NULL
for(i in 1:length(pred_in)){
  covars = pred_in[1:i]
  f = as.formula(paste(Response,paste(covars,collapse = "+"),sep = " ~ "))
  mod = coxph(f, data=dat_log_sel)
  C_covars = mod$concordance['concordance'] %>% as.numeric()
  Cum_C_stats = c(Cum_C_stats, C_covars)
}

## 07/02/2020:
## get leave-one-out CV results
## (in terms of deviance, i.e., partial likelihood)
cv_phmod_lasso3 = cv.glmnet(X, 
                            Surv(dat_log_sel$rebound_time_days_post_ati, 
                                 dat_log_sel$observed),
                            family = "cox", nfolds = 10, keep = T)

pred_in2 = NULL
LOO_deviance = NULL
for(l in cv_phmod_lasso3$lambda){
  coefficients = coef(phmod_lasso, s = l)
  active_index = which(coefficients != 0)
  #active_coefficients = coefficients[active_index]
  active_predictors = attr(coefficients,"Dimnames")[[1]][active_index]
  
  #cat(active_predictors, "\n")
  
  if(any(!active_predictors %in% pred_in2)){
    new_index = which(!active_predictors %in% pred_in2)
    pred_in2 = c(pred_in2, active_predictors[new_index])
    
    l_index = which(cv_phmod_lasso3$lambda == l)
    LOO_deviance = c(LOO_deviance, cv_phmod_lasso3$cvm[l_index])
  }
}


## put together a summary table of this thing
predictor_inclusion = data.frame(Inclusion_Rank = c(1:9),
                                 Predictor = pred_in,
                                 Coefficient = coef_sign,
                                 Rebound_Effect = rebound_effect,
                                 #Concordance = C_stats)
                                 Cum_concordance = Cum_C_stats,
                                 LOOCV_deviance = LOO_deviance)

#predictor_inclusion

knitr::kable(predictor_inclusion, digits = 4)
```

## On literature review

The general framework of parameter estimation for ODE-based models:

$$
\begin{align}
x'(t) &= f(t, x(t), \theta),\\
x(0) &= x_0,\\
y(t) &= h(x(t), x_0, \theta) + \epsilon(t).
\end{align}
$$
Here $x(t)$ is the "mechanistic" model and $y(t)$ is the observed data. The goal is to estimate $\theta$. 

Main approaches:

- Work with $\epsilon(t)$ to elicit a "likelihood", and then do MLE 
    - oftentimes it's reduced to nonlinear LS
    - but can be more complicated if, say, the model is hierarchical (e.g., mixed effects model)
- "Smooth" $y(t)$ to get an estimate of $x(t)$ and then work with $x'(t)$ directly
    - "nonparametric" (using splines) so somewhat more flexible
    - minimizing $\lVert \hat{x}'(t) - x'(t) \rVert_2$ can be easier and doesn't require knowing the initial conditions accurately


<!-- My plan is to read up more on: -->

<!-- - likelihood-free methods (e.g., ABC, ABC-SMC, and others) -->
<!-- - simulation-based methods (e.g., model-based proposals, particle filtering, etc.) -->
<!-- - approximation techniques (e.g., linear noise approximation) -->

<!-- ## Literature review (Cont'd) -->

## Estimation framework based on linear noise approximation and generalized least squares

Start with a continuous-time Markov Chain formulation, and for simplicity, only work with a 1-d process (multi-dim case is very similar).

Suppose that, for process $y(t)$, $a(y(t))$ is the instantaneous rate; i.e.,

$$
a(y(t)) \Delta t = \text{prob. of an event in infinitesimal interval } [t, t+\Delta t).
$$

Now we discretize the process by considering the process value at a time step $\tau$. Assume that $\tau$ satisfies the following two conditions:

1. $a(y(t))$ is approximately constant on $[t,t+\tau)$,
2. $a(y(t))\tau >> 1$.

(Note that these two conditions are easy to satisfy in a system very close to the thermodynamic limit, but can be hard to satisfy in a small-size population.)

**Condition 1** means that the event count on interval $[t,t+\tau)$, $\Delta y(t,\tau)$, can be approximated by a Poisson random variable with rate $a(y(t))\tau$.

**Condition 2** suggests that $E(\Delta y(t,\tau))$ is large enough that $\Delta y(t,\tau)$ can be approximated by $N(a(y(t))\tau, a(y(t))\tau)$. 

Therefore, we have
$$
y(t+\tau) \approx y(t) + a(y(t))\tau + N(0, a(y(t))\tau),
$$
and by induction,
$$
\begin{align}
y(n\tau) &\approx y(0) + \tau \sum_{k=0}^{n-1} a(y(k\tau)) + N\left(0, \tau \sum_{k=0}^{n-1} a(y(k\tau))\right)\\
&= E(y(n\tau)) + N(0, E(y(n\tau))).
\end{align}
$$
Here the second step is achieved assuming $y(0)=0$ (which can be done by subtracting a common "intercept" of the curve).

Now we can apply this approximation to the ODE framework. Assume that the observed data $y(t)$ is simply a perturbed value centered around the real system mean $x(t)$,
$$
y(t) = x(t; \theta) + \epsilon(t).
$$
Then we can assume that
$$
\epsilon(t) \sim N(0, \sigma^2(t))
$$
where $\sigma^2(t)$ scales by $x(t;\theta)$. If we allow some degree of dispersion, then we can parametrize it by
$$
\sigma^2(t) = \gamma x(t;\theta).
$$

Parameter estimation can be accomplished by iterative generalized least squares (iGLS) procedure:

First, get an initial estimate $\theta^{(1)}$ via
$$
\theta^{(1)} = \text{argmin}_{\theta} \sum_{i=1}^N\frac{(y(t_i)-x(t_i;\theta))^2}{x(t_i;\theta)},
$$
where $\theta$ on the denominator can be replaced by some other initial estimate $\theta^{(0)}$.

Then with $k=1:\text{maxIter}$, iteratively repeat the following two steps until convergence:
1. Estimate the variation scalar (the dispersion parameter) $\gamma$
$$
\gamma = \frac{1}{N-p} \sum_{i=1}^N\frac{(y(t_i)-x(t_i;\theta^{(k)}))^2}{x(t_i;\theta^{(k)})},
$$
where $p$ is the dimension of the parameter $\theta$.

2. Update parameter $\theta$ by weighted least squares
$$
\theta^{(k+1)} = \text{argmin}_{\theta} \sum_{i=1}^N\frac{(y(t_i)-x(t_i;\theta))^2}{\gamma x(t_i;\theta^{(k)})}.
$$

Major shortcoming of this method: it requires solving the ODE curve $x(t;\theta)$ repeatedly for any parameter setting $\theta$. BUT $x(t)$ can also be computed using the discrete approximation expression mentioned above, i.e., 
$$
x(t) = E(y(t)) = \text{sum of interval event rates}.
$$

## Simulation experiment (simple example)

Assume the following 1-d ODE system:
$$
\begin{align}
x'(t) &= \lambda x(t),\\
x(0) &= 1.
\end{align}
$$
That is, 
$$
x(t) = e^{\lambda t},
$$
where $\lambda$ is the parameter of interest. 

The stochastic version of this dynamic model would be a CTMC $y(t)$ with instantaneous rate function $\gamma y(t)$ and $y(0)=1$. This is a pure-birth process with brith rate $\gamma$.

The "true" process sequence is simulated from such a CTMC, and then discrete "observations" are obtained every $\tau$ time units. The iGLS procedure described above is applied to the discrete observed data to acquire an estimate of $\lambda$. This estimate is also compared with that obtained using nonlinear least squares (optimzation via Gauss-Newton algorithm).

The plot below shows estimation errors of the proposed framework (**Approx**) and **NLS**. All the observed datapoints are from the first 10 time units (to mimic the randomness of small-populations/initial dynamics), with ground truth $\lambda = 0.3, 0.5, 1$ and discrete time step $\tau = 0.2, 0.5, 1$. For each setting the experiment is repeated 20 times. 

```{r estimation Exponential, echo=F, warning=FALSE, message=F}
Res = readRDS("ODE_inference/Exponential_simulation_res.rds")

ggplot(data=Res, aes(x=method,y=error)) +
  geom_hline(yintercept = 0, size=1, color="gray") +
  geom_boxplot() +
  #geom_violin() +
  theme_bw(base_size = 14)+
  facet_grid(lambda~tau)
```



## References

Adams et al. (2005). HIV dynamics: Modeling, data analysis, and optimal treatment protocols. *Journal of computational and applied mathematics*. 

Wallace et al. (2012). Linear noise approximation is valid over limited times for any chemical systems that is sufficiently large. *IET systems biology*.
